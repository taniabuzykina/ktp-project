{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tif_files(directory_path, pattern):\n",
    "    \"\"\"\n",
    "    Read all TIFF files matching the pattern from directory\n",
    "    pattern: 'NDMI' or 'NDVI'\n",
    "    Returns: Dictionary with filenames as keys and numpy arrays as values\n",
    "    \"\"\"\n",
    "    # Get list of files in directory\n",
    "    try:\n",
    "        files = os.listdir(directory_path)\n",
    "        # Filter for TIFF files matching the pattern\n",
    "        tif_files = sorted([f for f in files if f.startswith(pattern) and f.endswith('.tif')])\n",
    "        \n",
    "        if not tif_files:\n",
    "            raise ValueError(f\"No {pattern} TIFF files found in {directory_path}\")\n",
    "            \n",
    "        data_dict = {}\n",
    "        for filename in tif_files:\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with rasterio.open(file_path) as src:\n",
    "                data = src.read(1)\n",
    "                data_dict[filename] = data\n",
    "                \n",
    "        print(f\"Loaded {len(data_dict)} {pattern} files from {directory_path}\")\n",
    "        return data_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pattern} files from {directory_path}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(ndmi_dir, ndvi_dir, precip_df):\n",
    "    \"\"\"\n",
    "    Prepare features from TIFF files and precipitation data\n",
    "    precip_df: DataFrame with columns [tiff_file, precipitation_sum, irrigation]\n",
    "    \"\"\"\n",
    "    # Read TIFF files\n",
    "    print(\"Reading TIFF files...\")\n",
    "    ndmi_dict = read_tif_files(ndmi_dir, 'NDMI')\n",
    "    ndvi_dict = read_tif_files(ndvi_dir, 'NDVI')\n",
    "    \n",
    "    # Function to extract dates from filename\n",
    "    def extract_dates(filename):\n",
    "        # Extract start date from NDMI_YYYY-MM-DD_to_YYYY-MM-DD.tif\n",
    "        start_date = filename.split('_')[1]\n",
    "        return start_date\n",
    "    \n",
    "    # Create mapping between NDMI and NDVI files using start dates\n",
    "    ndvi_mapping = {}\n",
    "    for ndvi_file in ndvi_dict.keys():\n",
    "        ndvi_date = extract_dates(ndvi_file)\n",
    "        ndvi_mapping[ndvi_date] = ndvi_file\n",
    "    \n",
    "    # Find extra NDVI file\n",
    "    ndmi_dates = set(extract_dates(f) for f in ndmi_dict.keys())\n",
    "    ndvi_dates = set(extract_dates(f) for f in ndvi_dict.keys())\n",
    "    extra_date = ndvi_dates - ndmi_dates\n",
    "    if extra_date:\n",
    "        extra_file = [f for f in ndvi_dict.keys() if extract_dates(f) in extra_date][0]\n",
    "        print(f\"Excluding extra NDVI file: {extra_file}\")\n",
    "    \n",
    "    # Sort precipitation data by date\n",
    "    precip_df = precip_df.sort_values('tiff_file')\n",
    "    \n",
    "    features = []\n",
    "    targets = []\n",
    "    dates = []\n",
    "    \n",
    "    print(\"\\nPreparing feature vectors...\")\n",
    "\n",
    "    # Iterate through precipitation data to ensure alignment\n",
    "    for i in range(1, len(precip_df)):\n",
    "        current_ndmi_file = precip_df.iloc[i]['tiff_file']\n",
    "        previous_ndmi_file = precip_df.iloc[i-1]['tiff_file']\n",
    "\n",
    "        \n",
    "        # Get corresponding NDVI files\n",
    "        current_date = extract_dates(current_ndmi_file)\n",
    "        previous_date = extract_dates(previous_ndmi_file)\n",
    "        \n",
    "        current_ndvi_file = ndvi_mapping.get(current_date)\n",
    "        previous_ndvi_file = ndvi_mapping.get(previous_date)\n",
    "\n",
    "        # print(previous_ndmi_file)\n",
    "        # print(previous_ndvi_file)\n",
    "        \n",
    "        # Check if we have all required data\n",
    "        if (current_ndmi_file in ndmi_dict and previous_ndmi_file in ndmi_dict and\n",
    "            current_ndvi_file and previous_ndvi_file):  # Check if NDVI files exist\n",
    "            print('Exists')\n",
    "        else:\n",
    "            print('Does not exist')\n",
    "            continue\n",
    "            \n",
    "    #         # Current values\n",
    "    #         ndmi_current = ndmi_dict[current_ndmi_file].flatten()\n",
    "    #         ndvi_current = ndvi_dict[current_ndvi_file].flatten()\n",
    "    #         precip = precip_df.iloc[i]['precipitation_sum']\n",
    "            \n",
    "    #         # Previous values\n",
    "    #         ndmi_prev = ndmi_dict[previous_ndmi_file].flatten()\n",
    "    #         ndvi_prev = ndvi_dict[previous_ndvi_file].flatten()\n",
    "            \n",
    "    #         # Combine features\n",
    "    #         feature_row = np.concatenate([\n",
    "    #             ndmi_current,\n",
    "    #             ndvi_current,\n",
    "    #             ndmi_prev,\n",
    "    #             ndvi_prev,\n",
    "    #             [precip]\n",
    "    #         ])\n",
    "            \n",
    "    #         features.append(feature_row)\n",
    "    #         targets.append(precip_df.iloc[i]['irrigation'])\n",
    "    #         dates.append(current_ndmi_file)\n",
    "    \n",
    "    # print(f\"\\nDataset summary:\")\n",
    "    # print(f\"Total samples: {len(features)}\")\n",
    "    # print(f\"Feature vector size: {features[0].shape[0]} elements\")\n",
    "    # print(f\"Date range: {dates[0]} to {dates[-1]}\")\n",
    "    \n",
    "    # return np.array(features), np.array(targets), dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_irrigation_model(features, targets):\n",
    "    # Time series split for validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 6,\n",
    "        'n_estimators': 100,\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'gamma': 0\n",
    "    }\n",
    "    \n",
    "    models = []\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in tscv.split(features):\n",
    "        X_train, X_val = features[train_idx], features[val_idx]\n",
    "        y_train, y_val = targets[train_idx], targets[val_idx]\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=10,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        pred = model.predict(X_val)\n",
    "        score = r2_score(y_val, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "        \n",
    "        models.append(model)\n",
    "        scores.append({'R2': score, 'RMSE': rmse})\n",
    "    \n",
    "    best_model_idx = np.argmax([s['R2'] for s in scores])\n",
    "    return models[best_model_idx], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_irrigation(model, features, current_irrigation):\n",
    "    optimal_irrigation = model.predict(features.reshape(1, -1))[0]\n",
    "    \n",
    "    # Apply constraints\n",
    "    min_irrigation = 0\n",
    "    max_irrigation = 50  # Adjust based on your requirements\n",
    "    \n",
    "    optimal_irrigation = np.clip(optimal_irrigation, min_irrigation, max_irrigation)\n",
    "    savings = current_irrigation - optimal_irrigation if optimal_irrigation < current_irrigation else 0\n",
    "    \n",
    "    return optimal_irrigation, savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndmi_dir = \"data/NDMI_Weekly_Exports/NDMI_Weekly_Exports_2019_2024\"\n",
    "ndvi_dir = \"data/NDVI_Weekly_Exports/NDVI_Weekly_Exports_2019_2024\"\n",
    "# Load precipitation and irrigation data\n",
    "precip_df = pd.read_csv('data/precipitation_sums.csv')  # Should contain tiff_file, precipitation_sum, irrigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing features...\n",
      "Reading TIFF files...\n",
      "Loaded 199 NDMI files from data/NDMI_Weekly_Exports/NDMI_Weekly_Exports_2019_2024\n",
      "Loaded 200 NDVI files from data/NDVI_Weekly_Exports/NDVI_Weekly_Exports_2019_2024\n",
      "Excluding extra NDVI file: NDVI_2019-07-02_to_2019-07-09.tif\n",
      "\n",
      "Preparing feature vectors...\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Does not exist\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n",
      "Exists\n"
     ]
    }
   ],
   "source": [
    "# Prepare features\n",
    "print(\"Preparing features...\")\n",
    "# features, targets, dates = prepare_features(ndmi_dir, ndvi_dir, precip_df)\n",
    "prepare_features(ndmi_dir, ndvi_dir, precip_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
